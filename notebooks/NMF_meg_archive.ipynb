{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with MEG dataset\n",
    "MEG dataset from THINGS initiative: https://openneuro.org/datasets/ds004212/versions/2.0.1\n",
    "### Training data partition\n",
    "The data is .fif file which needs to be converted into a rather easy to use .npy array.\n",
    "We will use mne library for this: https://mne.tools/dev/index.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collected extensively sampled object representations using magnetoencephalography (MEG). To this end, we drew on the THINGS database (Hebart et al., 2019), a richly-annotated database of 1,854 object concepts representative of the American English language which contains 26,107 manually-curated naturalistic object images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERP = event-related potential; signal with respect to some event\n",
    "VEP = ERP for visual stimuli\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27084 events, 281 time points, 271 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne, os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.image as mpimg\n",
    "import nimfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2039442/1096225509.py:4: RuntimeWarning: This filename (preprocessed) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(file_path, preload=False)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File does not exist: \"/home/asuryawanshi/Documents/Neural-Representations-using-Things-Database/MEG/notebooks/preprocessed\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mread_epochs(file_path, preload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m epochs\n\u001b[0;32m----> 6\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[43mload_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mload_epochs\u001b[0;34m(file_path, all_epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_epochs\u001b[39m(file_path,all_epochs \u001b[38;5;241m=\u001b[39m []):\n\u001b[0;32m----> 4\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m \u001b[43mmne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m epochs\n",
      "File \u001b[0;32m<decorator-gen-232>:12\u001b[0m, in \u001b[0;36mread_epochs\u001b[0;34m(fname, proj, preload, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/mne/epochs.py:4213\u001b[0m, in \u001b[0;36mread_epochs\u001b[0;34m(fname, proj, preload, verbose)\u001b[0m\n\u001b[1;32m   4195\u001b[0m \u001b[38;5;129m@verbose\u001b[39m\n\u001b[1;32m   4196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_epochs\u001b[39m(fname, proj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, preload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochsFIF\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read epochs from a fif file.\u001b[39;00m\n\u001b[1;32m   4198\u001b[0m \n\u001b[1;32m   4199\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4211\u001b[0m \u001b[38;5;124;03m        The epochs.\u001b[39;00m\n\u001b[1;32m   4212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEpochsFIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-233>:12\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, fname, proj, preload, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/mne/epochs.py:4262\u001b[0m, in \u001b[0;36mEpochsFIF.__init__\u001b[0;34m(self, fname, proj, preload, verbose)\u001b[0m\n\u001b[1;32m   4256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _path_like(fname):\n\u001b[1;32m   4257\u001b[0m     check_fname(\n\u001b[1;32m   4258\u001b[0m         fname\u001b[38;5;241m=\u001b[39mfname,\n\u001b[1;32m   4259\u001b[0m         filetype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4260\u001b[0m         endings\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-epo.fif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-epo.fif.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_epo.fif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_epo.fif.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   4261\u001b[0m     )\n\u001b[0;32m-> 4262\u001b[0m     fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43m_check_fname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmust_exist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   4263\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preload:\n\u001b[1;32m   4264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreload must be used with file-like objects\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<decorator-gen-0>:12\u001b[0m, in \u001b[0;36m_check_fname\u001b[0;34m(fname, overwrite, must_exist, name, need_dir, check_bids_split, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/mne/utils/check.py:274\u001b[0m, in \u001b[0;36m_check_fname\u001b[0;34m(fname, overwrite, must_exist, name, need_dir, check_bids_split, verbose)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not have read permissions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m must_exist:\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fname\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File does not exist: \"/home/asuryawanshi/Documents/Neural-Representations-using-Things-Database/MEG/notebooks/preprocessed\""
     ]
    }
   ],
   "source": [
    "channel_picks               = ['O','T','P']\n",
    "file_path = 'preprocessed'\n",
    "def load_epochs(file_path,all_epochs = []):\n",
    "    epochs = mne.read_epochs(file_path, preload=False)\n",
    "    return epochs\n",
    "epochs = load_epochs(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = channel_picks[0]\n",
    "print(name, channel_picks)\n",
    "# ctf_layout = mne.find_layout(epochs.info)\n",
    "picks_epochs = [epochs.ch_names[i] for i in np.where([s[2]==name for s in epochs.ch_names])[0]]\n",
    "ep1 = epochs[epochs.metadata['trial_type']=='exp']  \n",
    "ep1.load_data() \n",
    "ep1.pick_channels(ch_names=picks_epochs); # supress output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOS_CONCEPTS = 1854\n",
    "NOS_IMAGE_PER_CONCEPT = 12\n",
    "NOS_CHANNELS_OPT = 39\n",
    "NOS_TIME_POINTS = 281\n",
    "time_points = epochs.times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load concept epochs if they already exist\n",
    "# if os.path.exists('concept_epochs.npy'):\n",
    "#     concept_epochs = np.load('concept_epochs.npy')\n",
    "# else:\n",
    "# Initialize the concept_epochs array with zeros\n",
    "concept_epochs = np.zeros((NOS_IMAGE_PER_CONCEPT, NOS_CONCEPTS, NOS_TIME_POINTS, NOS_CHANNELS_OPT))\n",
    "\n",
    "# Extract all data matching the condition in one go\n",
    "indices = ep1.metadata['category_nr'].values - 1  # Adjust index (assuming category_nr starts at 1)\n",
    "concept_epochs[:, indices, :, :] = ep1._data.transpose(0, 2, 1)\n",
    "    \n",
    "    # # Save the array\n",
    "    # np.save('concept_epochs.npy', concept_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_epochs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_indices = (epochs._raw_times > 0.07) & (epochs._raw_times < 0.370)\n",
    "concept_epochs = concept_epochs[:, :, filtered_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_concept_epochs = concept_epochs.mean(axis = 0)\n",
    "average_concept_epochs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scored_epochs = np.zeros_like(average_concept_epochs)\n",
    "for channel in range(NOS_CHANNELS_OPT):\n",
    "    mean = average_concept_epochs[:,channel,:].mean()\n",
    "    stdev = average_concept_epochs[:,channel,:].std()\n",
    "    z_scored_epochs[:,channel,:] = (average_concept_epochs[:,channel,:] - mean) / stdev\n",
    "z_scored_epochs+= abs(z_scored_epochs.min()) # make all values positive  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(z_scored_epochs[:,2,:].flatten(), bins=100)\n",
    "plt.title('Histogram of z-scored epochs for a sample channel')\n",
    "plt.xlabel('amplitude')\n",
    "plt.ylabel('# of occurrences') \n",
    "plt.show()\n",
    "z_scored_epochs.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = z_scored_epochs.reshape(NOS_CONCEPTS, average_concept_epochs.shape[1]*NOS_CHANNELS_OPT)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bnmf(V: np.ndarray, k: int):\n",
    "    bnmf = nimfa.Bd(V, seed='random_c', rank=k, max_iter=500, min_residuals=1e-4, alpha=np.zeros((V.shape[0], k)),\n",
    "                        beta=np.zeros((k, V.shape[1])), theta=.0, k=.0, sigma=1., skip=100, stride=1,\n",
    "                        n_w=np.zeros((k, 1)), n_h=np.zeros((k, 1)), n_run=1, n_sigma=False)\n",
    "    bnmf_fit = bnmf()\n",
    "    W = bnmf_fit.basis()\n",
    "    H = bnmf_fit.coef()\n",
    "    return np.array(W), np.array(H)\n",
    "\n",
    "def compute_evar_all(V: np.ndarray, W: np.ndarray, H: np.ndarray):\n",
    "    V_hat = np.dot(W, H)\n",
    "    rss = np.sum(np.asarray(V_hat - V)**2)\n",
    "    evar_all = 1. - rss / (V*V).sum()\n",
    "    return evar_all\n",
    "\n",
    "def compute_evar_indiv(V: np.ndarray, W: np.ndarray, H: np.ndarray, d: int):\n",
    "    V_hat_d = np.outer(W[:, d], H[d, :])\n",
    "    rss = np.sum(np.asarray(V_hat_d - V)**2)\n",
    "    evar_indiv = 1. - rss / (V*V).sum()\n",
    "    return evar_indiv\n",
    "\n",
    "def compute_evar_unique(V: np.ndarray, W: np.ndarray, H: np.ndarray, d: int, evar_all: float):\n",
    "    V_hat_wo_d = np.dot(W[:, np.arange(W.shape[1]) != d], H[np.arange(H.shape[0]) != d, :])\n",
    "    rss = np.sum(np.asarray(V_hat_wo_d - V)**2)\n",
    "    evar_rest = 1. - rss / (V*V).sum()\n",
    "    evar_unique = evar_all - evar_rest\n",
    "    return evar_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X = W \\times H$$\n",
    "1854 is the number of data points and time*channels is the dimensionality of each data point.\n",
    "$W$ is the matrix of basis vectors and $H$ is the coefficient of activity of this basis vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "def compute_log_likelihood(V: np.ndarray, W: np.ndarray, H: np.ndarray):\n",
    "    \"\"\"Compute log-likelihood.\"\"\"\n",
    "    V_hat = np.dot(W, H)\n",
    "    err_std_dev = np.sqrt(np.var(V - V_hat))\n",
    "    log_likelihood = st.norm.logpdf(V, loc=V_hat, scale=err_std_dev).sum()\n",
    "    return log_likelihood\n",
    "\n",
    "def compute_aic(V: np.ndarray, W: np.ndarray, H: np.ndarray):\n",
    "    \"\"\"Compute AIC.\"\"\"\n",
    "    log_likelihood = compute_log_likelihood(V, W, H)\n",
    "    n_free_params = np.count_nonzero(W) + np.count_nonzero(H) + 1\n",
    "    aic = 2 * n_free_params - 2 * log_likelihood\n",
    "    return aic, n_free_params\n",
    "\n",
    "def compute_bic(V: np.ndarray, W: np.ndarray, H: np.ndarray):\n",
    "    \"\"\"Compute BIC.\"\"\"\n",
    "    log_likelihood = compute_log_likelihood(V, W, H)\n",
    "    I, J = V.shape\n",
    "    n_samples = I * J\n",
    "    n_free_params = np.count_nonzero(W) + np.count_nonzero(H) + 1\n",
    "    bic = np.log(n_samples) * n_free_params - 2 * log_likelihood\n",
    "    return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_components = 250 # 3 mins for 30 components \n",
    "BAYESIAN = 1\n",
    "\n",
    "# Directory to save/load the matrices\n",
    "directory = 'NMF_matrices'  \n",
    "\n",
    "# Check if W, H exists in folder and load them\n",
    "if os.path.isfile(f'{directory}/W_{nmf_components}.npy') and os.path.isfile(f'{directory}/H_{nmf_components}.npy'):\n",
    "    W = np.load(f'{directory}/W_{nmf_components}.npy')\n",
    "    H = np.load(f'{directory}/H_{nmf_components}.npy')\n",
    "else:\n",
    "    if BAYESIAN:\n",
    "        W, H = fit_bnmf(X, nmf_components)\n",
    "        X_pred = np.dot(W, H)\n",
    "    else:\n",
    "        model = NMF(n_components=nmf_components)\n",
    "        W = model.fit_transform(X)\n",
    "        H = model.components_\n",
    "        X_pred = np.dot(W, H)\n",
    "    np.save(f'{directory}/W_{nmf_components}.npy', W)\n",
    "    np.save(f'{directory}/H_{nmf_components}.npy', H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_evar_all(X, W, H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_dim = [2, 5, 10, 15, 20, 50, 100, 200, 250, 300]\n",
    "W_dict = {} \n",
    "H_dict = {} \n",
    "\n",
    "for dim in array_dim:\n",
    "    if os.path.isfile(f'{directory}/W_{dim}.npy') and os.path.isfile(f'{directory}/H_{dim}.npy'):\n",
    "        W_dict[dim] = np.load(f'{directory}/W_{dim}.npy')\n",
    "        H_dict[dim] = np.load(f'{directory}/H_{dim}.npy')\n",
    "# # load matrices from directory\n",
    "# W_2 = np.load(f'{directory}/W_2.npy')\n",
    "# H_2 = np.load(f'{directory}/H_2.npy')\n",
    "# W_5 = np.load(f'{directory}/W_5.npy')   \n",
    "# H_5 = np.load(f'{directory}/H_5.npy')\n",
    "# W_10 = np.load(f'{directory}/W_10.npy')\n",
    "# H_10 = np.load(f'{directory}/H_10.npy')\n",
    "# W_15 = np.load(f'{directory}/W_15.npy')\n",
    "# H_15 = np.load(f'{directory}/H_15.npy')\n",
    "# W_20 = np.load(f'{directory}/W_20.npy')\n",
    "# H_20 = np.load(f'{directory}/H_20.npy')\n",
    "# W_50 = np.load(f'{directory}/W_50.npy')\n",
    "# H_50 = np.load(f'{directory}/H_50.npy')     \n",
    "# W_100 = np.load(f'{directory}/W_100.npy')\n",
    "# H_100 = np.load(f'{directory}/H_100.npy')   \n",
    "# W_200 = np.load(f'{directory}/W_200.npy')                       \n",
    "# H_200 = np.load(f'{directory}/H_200.npy')\n",
    "# W_250 = np.load(f'{directory}/W_250.npy')\n",
    "# H_250 = np.load(f'{directory}/H_250.npy')\n",
    "# W_300 = np.load(f'{directory}/W_300.npy')\n",
    "# H_300 = np.load(f'{directory}/H_300.npy')\n",
    "\n",
    "BIC_array = np.zeros(len(array_dim))\n",
    "AIC_array = np.zeros(len(array_dim))\n",
    "explained_variance = np.zeros(len(array_dim))   \n",
    "for i, dim in enumerate(array_dim):\n",
    "    W = W_dict[dim]\n",
    "    H = H_dict[dim]\n",
    "    BIC_array[i] = compute_bic(X, W, H)\n",
    "    AIC_array[i] = compute_aic(X, W, H)[0]\n",
    "    explained_variance[i] = compute_evar_all(X, W, H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot AIC and BIC\n",
    "plt.plot(array_dim, AIC_array, label='AIC')     \n",
    "plt.plot(array_dim, BIC_array, label='BIC')    \n",
    "plt.plot(array_dim, explained_variance, label='Explained Variance')         \n",
    "plt.xlabel('Number of components')          \n",
    "plt.ylabel('Information Criterion')                 \n",
    "plt.legend()        \n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(array_dim, explained_variance, label='Explained Variance')   \n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained Variance')            \n",
    "plt.legend()    \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_dim = [2, 5, 10, 15, 20, 50, 100, 200, 250, 300]\n",
    "\n",
    "# compute BIC \n",
    "print(f\"BIC of 2 is {compute_bic(X, W_2, H_2)}\")    \n",
    "print(f\"BIC of 5 is {compute_bic(X, W_5, H_5)}\")\n",
    "print(f\"BIC of 10 is {compute_bic(X, W_10, H_10)}\")  \n",
    "print(f\"BIC of 15 is {compute_bic(X, W_15, H_15)}\")               \n",
    "print(f\"BIC of 20 is {compute_bic(X, W_20, H_20)}\")             \n",
    "print(f\"BIC of 50 is {compute_bic(X, W_50, H_50)}\")\n",
    "print(f\"BIC of 100 is {compute_bic(X, W_100, H_100)}\")\n",
    "print(f\"BIC of 200 is {compute_bic(X, W_200, H_200)}\")\n",
    "print(f\"BIC of 250 is {compute_bic(X, W_250, H_250)}\")\n",
    "print(f\"BIC of 300 is {compute_bic(X, W_300, H_300)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot bic's \n",
    "bic = [compute_bic(X, W_2, H_2), compute_bic(X, W_5, H_5), compute_bic(X, W_10, H_10), compute_bic(X, W_15, H_15), compute_bic(X, W_20, H_20), compute_bic(X, W_50, H_50), compute_bic(X, W_100, H_100), compute_bic(X, W_200, H_200), compute_bic(X, W_250, H_250), compute_bic(X, W_300, H_300)]\n",
    "plt.plot([2, 5, 10, 15, 20, 50, 100, 200,250, 300], bic)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('BIC')\n",
    "plt.title('BIC vs number of components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices which has max value on W\n",
    "sorted_indices = np.argsort(W[:,0])\n",
    "sorted_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_components_vs_category = np.zeros((nmf_components, 10))\n",
    "for i in range(nmf_components):\n",
    "    sorted_indices = np.argsort(W[:,i])\n",
    "    print(f'Categories which are best on component {i} are: {sorted_indices[-10:]}')\n",
    "    nmf_components_vs_category[i,:] = sorted_indices[-10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in range(nmf_components):\n",
    "    print(f\"Component {component} is best loaded by the following image categories:\")\n",
    "    for i in range(10):\n",
    "        category_nr = nmf_components_vs_category[component, i] \n",
    "        # Get the image paths for the given category_nr\n",
    "        image_paths = epochs.metadata[(epochs.metadata['category_nr'] == category_nr+1) & (epochs.metadata['trial_type'] == 'exp')]['image_path']\n",
    "        \n",
    "        if not image_paths.empty:\n",
    "            # Print the folder name only for the first occurrence\n",
    "            first_image_path = image_paths.iloc[0]\n",
    "            folder_name = os.path.dirname(first_image_path)\n",
    "            print(f\"Category {category_nr} - {folder_name}\")\n",
    "        else:\n",
    "            print(f\"Category {category_nr} - No image paths found \")\n",
    "            break\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot images for the top 5 categories for each component   \n",
    "for component in range(nmf_components):\n",
    "    if component > 20:\n",
    "        break\n",
    "    print(f\"Component {component} is best loaded by the following image categories:\")\n",
    "    fig, axs = plt.subplots(1, 10, figsize=(20, 20))\n",
    "    for i in range(10):\n",
    "        category_nr = nmf_components_vs_category[component, i] \n",
    "        # Get the image paths for the given category_nr\n",
    "        image_paths = epochs.metadata[(epochs.metadata['category_nr'] == category_nr) & (epochs.metadata['trial_type'] == 'exp')]['image_path']\n",
    "        image_path = image_paths.iloc[0]\n",
    "        img = mpimg.imread(image_path)\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].set_title(f\"Category {category_nr}\")\n",
    "        axs[i].axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
